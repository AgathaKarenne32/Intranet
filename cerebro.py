import os
import sys
# Tenta importar o Llama, se falhar avisa o usu√°rio
try:
    from llama_cpp import Llama
except ImportError:
    print("‚ùå Erro: Biblioteca llama-cpp-python n√£o instalada.")
    sys.exit()

class CerebroDigital:
    def __init__(self):
        print("\nüß† [BOOT] Inicializando Qwen 7B (GGUF)...")
        
        # Caminho exato do novo arquivo
        nome_arquivo = "Qwen2.5-7B-Instruct-Q4_K_M.gguf"
        caminho_modelo = os.path.join("modelo_llm_7b_gguf", nome_arquivo)
        
        if not os.path.exists(caminho_modelo):
            print(f"‚ùå ERRO CR√çTICO: N√£o achei o arquivo {nome_arquivo}")
            print(f"   Esperava em: {caminho_modelo}")
            print("   Rode o 'baixar_llm.py' novamente.")
            self.model = None
            return

        try:
            # Carrega o modelo
            # n_gpu_layers=0 garante que vai rodar s√≥ na CPU e RAM
            self.model = Llama(
                model_path=caminho_modelo,
                n_ctx=4096,      # Contexto (leitura) de 4096 tokens
                n_threads=6,     # Usa mais n√∫cleos do seu processador
                n_gpu_layers=0,  # For√ßa CPU
                verbose=False
            )
            print("   ‚úÖ C√©rebro 7B Carregado na Mem√≥ria!")
            
        except Exception as e:
            print(f"   ‚ùå Erro ao carregar LlamaCPP: {e}")
            self.model = None

    def pensar(self, pergunta, contextos):
        if not self.model: return "Erro: C√©rebro n√£o iniciou."

        texto_contexto = "\n\n".join(contextos)
        
        prompt_sistema = (
            "Voc√™ √© um Auditor Federal S√™nior. "
            "Analise o contexto jur√≠dico abaixo e responda √† pergunta com precis√£o. "
            "Cite a fonte (Instru√ß√£o Normativa, Lei, etc) sempre que poss√≠vel."
        )
        
        messages = [
            {"role": "system", "content": prompt_sistema},
            {"role": "user", "content": f"Contexto:\n{texto_contexto}\n\nPergunta: {pergunta}"}
        ]
        
        print("   ü§î O Auditor 7B est√° pensando...")
        
        output = self.model.create_chat_completion(
            messages=messages,
            max_tokens=1024,
            temperature=0.1
        )
        
        return output['choices'][0]['message']['content']